{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Training Modules","text":"<p>Self-paced training for platform engineering tools and practices. Each module includes hands-on exercises you can complete on your own machine.</p>"},{"location":"#available-modules","title":"Available Modules","text":"Module Description Prerequisites Git 101 Version control fundamentals\u2014commits, branches, merges, remotes None Puppet 101 Puppet fundamentals None Puppet 201 Advanced Puppet topics Puppet 101, Git 101"},{"location":"#how-to-use-these-modules","title":"How to Use These Modules","text":"<ol> <li>Check prerequisites. Each module lists required tools and prior modules. Install what you need before starting.</li> <li>Work through sections in order. Concepts build on each other.</li> <li>Use a scratch environment. Create throwaway repos and VMs so you can experiment freely.</li> <li>Break things on purpose. The best way to learn recovery is to need it.</li> </ol>"},{"location":"#tools-reference","title":"Tools Reference","text":"<p>Not sure how to install something? Check the Tools section for installation guides and configuration tips.</p>"},{"location":"ai-disclaimer/","title":"AI Disclaimer","text":"<p>This disclaimer was 100% written by a Grug Brained Developer.</p>"},{"location":"ai-disclaimer/#why-have-this-page-lucass-responsible-ai-use-soapbox","title":"Why have this page? (Lucas's Responsible AI Use Soapbox)","text":"<p>As Uncle Ben once said, \"with great power comes great responsibility\". It's super easy nowadays to tell an AI to write the next great play that will rival Shakespeare. That AI will gleefully dump out a metric ton of seemingly good content. You read the first few paragraphs and think they sound great! Now, you want to get some peer feedback on your magnum opus, (or maybe you just want to humble brag that you coerced technology into making something totally novel), so you plop that baby into a Word doc and send it off to your friend.</p> <p>Ask yourself, \"How much time did it take me to write this, and how much time will it take them to review it? Did you just absolve yourself of the responsibility of ownership by saddling your friend with it instead?</p> <p>If you haven't read and approved, word for word, the entire content of whatever you're creating with AI, before sending it out to the folks who need it, you are the problem. Respect eachother's time. Friends don't let friends publish AI slop.</p> <p>Use AI as a tool, don't defer to it.</p>"},{"location":"ai-disclaimer/#why-are-you-yelling-at-me","title":"Why are you yelling at me?","text":"<p>As you can see from the above rant, I am not Claude, ChatGPT, or Gemini. I don't have perfect pacing or formatting, my words come out rambly and informal.</p> <p>This is good. This is human. This is the sign of someone who has created content with care and intent.</p> <p>This is evidence that the author was passionate enough to spend more time than expected trying to communicate this information in a relatable and captivating way.</p>"},{"location":"ai-disclaimer/#the-actual-reason-for-this-page","title":"The actual reason for this page","text":"<p>If you've read this far, it means that you probably care enough to know that this site (with the exception of this page) was mostly generated using AI (Claude specifically).</p> <p>However</p> <p>I have read every single byte of text that was placed in each file. I have made numerous manual edits so these materials aren't just another pile of slop. I hope that after reading this page, you understand why that matters, and I hope that you take my words to heart and practice responsible AI usage so that we can all live happily ever after.</p> <p>Thanks for coming to my TED Talk <code>&lt;3</code></p>"},{"location":"concepts/","title":"Concepts","text":"<p>Background reading on the ideas and practices that shape modern IT operations. These pages provide context and history\u2014no hands-on exercises, just the \"why\" behind how we work.</p>"},{"location":"concepts/#available-concepts","title":"Available Concepts","text":"Concept Description Infrastructure as Code Managing infrastructure through code instead of manual processes Version Control Tracking and managing changes to files over time"},{"location":"concepts/infrastructure-as-code/","title":"Infrastructure as Code","text":"<p>Infrastructure as Code (IaC) is the practice of managing and provisioning infrastructure through machine-readable definition files rather than manual processes or interactive configuration tools.</p>"},{"location":"concepts/infrastructure-as-code/#a-brief-history","title":"A Brief History","text":""},{"location":"concepts/infrastructure-as-code/#the-before-times-manual-administration","title":"The Before Times: Manual Administration","text":"<p>In the early days of system administration, every server was a unique snowflake. Administrators would SSH into machines, run commands, edit configuration files, and hope they remembered what they did. Documentation, if it existed at all, was a Word document or wiki page that was perpetually out of date.</p> <p>This worked when you had a handful of servers. A skilled admin could keep the state of a few dozen machines in their head, and recovering from problems meant drawing on experience and institutional knowledge.</p> <p>But this approach had fundamental problems:</p> <ul> <li>Configuration drift. Two servers that started identical would slowly diverge as administrators made one-off changes, applied patches at different times, or troubleshot issues in slightly different ways.</li> <li>The documentation gap. Written documentation required discipline to maintain and was almost always incomplete or outdated. The real documentation lived in people's heads.</li> <li>Knowledge silos. When experienced administrators left, their knowledge left with them. New team members faced archeological expeditions through undocumented systems.</li> <li>Unreliable recovery. Rebuilding a server from scratch meant remembering every manual step, every config file tweak, every package that was installed. In practice, this meant nobody wanted to rebuild anything.</li> </ul>"},{"location":"concepts/infrastructure-as-code/#the-rise-of-scripting","title":"The Rise of Scripting","text":"<p>The first response to these problems was scripting. Instead of typing commands interactively, administrators wrote shell scripts to automate common tasks. This was a significant improvement\u2014scripts could be saved, shared, and rerun.</p> <p>But scripts had limitations:</p> <ul> <li>Procedural thinking. Scripts describe how to do something, not what the end result should be. Run a script twice and you might get different results depending on the starting state.</li> <li>Error handling complexity. Making scripts robust against failures\u2014network timeouts, package conflicts, partial completions\u2014required significant effort.</li> <li>No built-in idempotency. If a script failed halfway through, running it again might duplicate work or fail in new ways.</li> </ul>"},{"location":"concepts/infrastructure-as-code/#configuration-management-emerges","title":"Configuration Management Emerges","text":"<p>In the mid-2000s, a new generation of tools emerged to address these limitations. CFEngine had existed since 1993, but tools like Puppet (2005), Chef (2009), and later Ansible (2012) and Salt (2011) brought configuration management to the mainstream.</p> <p>These tools introduced key concepts:</p> <ul> <li>Declarative configuration. Instead of describing steps, you describe the desired end state. \"This server should have nginx installed and running.\" The tool figures out how to get there.</li> <li>Idempotency. Apply the same configuration multiple times and you get the same result. If nginx is already installed and running, the tool does nothing.</li> <li>Convergence. Systems continuously move toward their defined state. If someone manually changes a configuration, the tool changes it back.</li> <li>Abstraction. High-level resources (packages, services, files) hide platform-specific details. The same configuration can work across different operating systems.</li> </ul>"},{"location":"concepts/infrastructure-as-code/#cloud-and-the-explosion-of-scale","title":"Cloud and the Explosion of Scale","text":"<p>The rise of cloud computing in the late 2000s and 2010s changed the game again. Suddenly, infrastructure wasn't just configuration\u2014it was the servers themselves. You could spin up hundreds of virtual machines with an API call.</p> <p>This drove the development of provisioning tools that extended IaC principles to infrastructure provisioning itself. Now you could define not just how servers should be configured, but what servers should exist, what networks they should be connected to, and what security policies should apply.</p> <p>The scale also forced a philosophical shift. When you have thousands of servers, you can't treat them as pets with names and individual care. They become cattle\u2014identical, replaceable, and disposable. If a server misbehaves, you don't debug it; you destroy it and spin up a new one.</p>"},{"location":"concepts/infrastructure-as-code/#core-principles","title":"Core Principles","text":""},{"location":"concepts/infrastructure-as-code/#declarative-over-procedural","title":"Declarative Over Procedural","text":"<p>The most important principle of IaC is declarative configuration. You describe what you want, not how to get there.</p> <p>Procedural approach (shell script):</p> <pre><code>if ! rpm -q nginx; then\n    yum install -y nginx\nfi\nif ! systemctl is-active nginx; then\n    systemctl start nginx\nfi\nif ! systemctl is-enabled nginx; then\n    systemctl enable nginx\nfi\n</code></pre> <p>Declarative approach (Puppet):</p> <pre><code>package { 'nginx':\n  ensure =&gt; installed,\n}\nservice { 'nginx':\n  ensure =&gt; running,\n  enable =&gt; true,\n}\n</code></pre> <p>The declarative version is shorter, but that's not the point. The point is that it describes the goal without encoding assumptions about the current state. The tool handles the logic of \"if not already installed, install it.\"</p>"},{"location":"concepts/infrastructure-as-code/#idempotency","title":"Idempotency","text":"<p>An operation is idempotent if applying it multiple times produces the same result as applying it once. This is crucial for IaC because configurations are applied repeatedly\u2014on schedule, after changes, during recovery.</p> <p>Idempotency means you can safely run your configuration management at any time without fear of side effects. If everything is already in the desired state, nothing happens. If something has drifted, it gets corrected.</p>"},{"location":"concepts/infrastructure-as-code/#version-control-as-the-source-of-truth","title":"Version Control as the Source of Truth","text":"<p>IaC without version control is incomplete. When infrastructure is defined in code, that code should live in a version control system like Git. This provides:</p> <ul> <li>History. Every change is recorded with who made it, when, and why.</li> <li>Auditability. You can trace any configuration back to the commit that introduced it.</li> <li>Rollback. Reverting to a previous state means reverting to a previous commit.</li> <li>Collaboration. Multiple people can work on infrastructure using branches and pull requests.</li> <li>Review. Changes can be reviewed before they're applied, catching problems early.</li> </ul> <p>The version control repository becomes the single source of truth. The question \"what should this server look like?\" is answered by the code in the repository, not by inspecting the server itself.</p>"},{"location":"concepts/infrastructure-as-code/#immutable-infrastructure","title":"Immutable Infrastructure","text":"<p>An evolution of IaC principles is immutable infrastructure\u2014the idea that servers should never be modified after they're created. Instead of updating a running server, you build a new image with the changes and replace the old server entirely.</p> <p>This approach:</p> <ul> <li>Eliminates configuration drift entirely</li> <li>Makes rollback trivial (deploy the previous image)</li> <li>Ensures development, staging, and production are truly identical</li> <li>Simplifies debugging (the server matches exactly what was tested)</li> </ul> <p>Immutable infrastructure is most practical in cloud environments where creating and destroying servers is fast and cheap. It's enabled by image-building tools and container technologies.</p>"},{"location":"concepts/infrastructure-as-code/#benefits-and-tradeoffs","title":"Benefits and Tradeoffs","text":""},{"location":"concepts/infrastructure-as-code/#benefits","title":"Benefits","text":"<p>Consistency. Every server built from the same code is identical. No more \"works on my machine\" or mysterious differences between environments.</p> <p>Speed. Provisioning a new server takes minutes instead of hours or days. Scaling up means running more instances of the same code.</p> <p>Reliability. Automated processes don't forget steps or make typos. Recovery from failures is faster because rebuilding is automated.</p> <p>Collaboration. Infrastructure changes go through the same review processes as application code. Knowledge is captured in code, not trapped in individuals' heads.</p> <p>Compliance. IaC provides an audit trail of every change. You can prove that systems are configured according to policy because the policy is encoded in version-controlled code.</p>"},{"location":"concepts/infrastructure-as-code/#tradeoffs","title":"Tradeoffs","text":"<p>Learning curve. IaC tools have their own languages, concepts, and best practices. Teams need time to build expertise.</p> <p>Upfront investment. Writing good IaC takes longer than making a quick manual change. The payoff comes with scale and over time.</p> <p>Complexity. IaC adds layers of abstraction. When something goes wrong, you need to understand both the infrastructure and the tools managing it.</p> <p>State management. Some IaC tools maintain state files that must be carefully managed. State drift or corruption can cause serious problems.</p> <p>Not everything fits. Some infrastructure components\u2014legacy systems, one-off configurations, vendor appliances\u2014may not fit neatly into IaC models.</p>"},{"location":"concepts/infrastructure-as-code/#cultural-implications","title":"Cultural Implications","text":"<p>IaC isn't just a technical practice; it's a cultural shift. It requires thinking about infrastructure differently:</p> <ul> <li>Servers are disposable. Don't fix a broken server; replace it with a new one built from code.</li> <li>Changes go through process. No more SSH-ing in to make a quick fix. Changes go through version control and review.</li> <li>Documentation is code. The authoritative description of your infrastructure is the code that builds it.</li> <li>Everyone can contribute. When infrastructure is code, developers can propose infrastructure changes using the same tools they use for application code.</li> </ul> <p>This cultural shift can be challenging. Experienced administrators may resist losing direct access to systems. Processes slow down when every change needs a pull request. But organizations that embrace these changes find their infrastructure becomes more reliable, more scalable, and more understandable over time.</p>"},{"location":"concepts/version-control/","title":"Version Control","text":"<p>Version control is the practice of tracking and managing changes to files over time. It lets you record the history of a project, collaborate with others without overwriting each other's work, and return to any previous state when something goes wrong.</p>"},{"location":"concepts/version-control/#a-brief-history","title":"A Brief History","text":""},{"location":"concepts/version-control/#the-original-version-control-filenames","title":"The Original Version Control: Filenames","text":"<p>Before version control systems existed, people tracked changes the obvious way: copying files and renaming them.</p> <pre><code>report.doc\nreport_v2.doc\nreport_v2_final.doc\nreport_v2_final_FINAL.doc\nreport_v2_final_FINAL_reviewed.doc\n</code></pre> <p>This approach has obvious problems. Which file is actually current? What changed between versions? Who made which changes? What if two people edit different \"final\" copies and need to combine their work?</p> <p>For solo work on simple documents, filename versioning sort of works. For anything involving collaboration, multiple files, or the need to understand what changed over time, it falls apart quickly.</p>"},{"location":"concepts/version-control/#local-version-control","title":"Local Version Control","text":"<p>The first generation of version control systems solved the \"which version is current\" problem by automating the process of saving snapshots. Tools like RCS (Revision Control System, 1982) stored a history of changes in a local database. You could check out a file, edit it, and check it back in with a description of what changed.</p> <p>RCS tracked individual files, storing each version as a delta\u2014just the differences from the previous version. This was efficient for storage and let you see the history of any single file. But it had significant limitations:</p> <ul> <li>Single-user focus. RCS locked files during editing. Only one person could work on a file at a time.</li> <li>File-level tracking. Projects are made of related files, but RCS tracked each file independently. There was no concept of \"the state of the project at a point in time.\"</li> <li>Local only. The version history lived on one machine. No built-in way to share or synchronize with others.</li> </ul>"},{"location":"concepts/version-control/#centralized-version-control","title":"Centralized Version Control","text":"<p>The next generation addressed collaboration by introducing a central server. Systems like CVS (Concurrent Versions System, 1990) and later Subversion (SVN, 2000) stored the version history on a server that everyone connected to.</p> <p>This model introduced important concepts:</p> <ul> <li>Repository. A central location storing the complete history of a project.</li> <li>Working copy. A local checkout of files that you edit. Changes are committed back to the repository.</li> <li>Concurrent editing. Multiple people could work on the same files simultaneously. The system would merge changes or flag conflicts.</li> <li>Project-level commits. Changes to multiple files could be committed together as a single logical unit.</li> </ul> <p>Centralized systems made collaborative software development practical. Teams could work together on the same codebase, track who changed what, and maintain a shared history. Subversion in particular became the dominant version control system in the 2000s.</p> <p>But centralized systems had their own limitations:</p> <ul> <li>Network dependency. Most operations required a connection to the central server. Committing, viewing history, comparing versions\u2014all needed network access.</li> <li>Single point of failure. If the server went down, nobody could commit or access history. If the server was lost without backups, the history was gone.</li> <li>Branching pain. Branches were technically possible but often discouraged because merging was difficult and error-prone.</li> <li>Commit friction. Because commits went directly to the shared server, developers tended to make fewer, larger commits. Work in progress stayed local and untracked.</li> </ul>"},{"location":"concepts/version-control/#distributed-version-control","title":"Distributed Version Control","text":"<p>The current generation of version control systems\u2014Git (2005), Mercurial (2005), and others\u2014took a fundamentally different approach: distribution.</p> <p>In a distributed system, every developer has a complete copy of the repository, including the full history. You don't check out a working copy from a server; you clone the entire repository to your machine. Commits are local operations. Synchronizing with others is a separate step.</p> <p>This architecture has profound implications:</p> <ul> <li>Speed. Almost everything is local. Commits, diffs, history browsing, branch creation\u2014all happen instantly without network round trips.</li> <li>Offline capability. You can work, commit, branch, and explore history without any network connection. Synchronize when convenient.</li> <li>No single point of failure. Every clone is a full backup. If any server dies, any clone can restore it.</li> <li>Cheap branching. Branches are lightweight local operations. Creating a branch takes milliseconds. This changes how people work\u2014branches become a routine tool rather than a heavyweight process.</li> <li>Flexible workflows. There's no mandated central server. Teams can organize however they want: central repository, multiple repositories, peer-to-peer, hierarchical review chains.</li> </ul> <p>Git emerged from the Linux kernel development community after a dispute with their previous version control provider. Linus Torvalds designed it for the specific needs of a large, distributed open source project: speed, data integrity, and support for non-linear development with thousands of parallel branches.</p> <p>Git won. By the mid-2010s, it had become the dominant version control system across the software industry. The rise of GitHub (2008) accelerated this by adding a social layer\u2014pull requests, issues, profiles\u2014that made collaboration easier and more visible.</p>"},{"location":"concepts/version-control/#core-concepts","title":"Core Concepts","text":""},{"location":"concepts/version-control/#snapshots-not-diffs","title":"Snapshots, Not Diffs","text":"<p>Some version control systems store changes as a series of diffs\u2014the differences between each version and the next. Git takes a different approach: each commit is a snapshot of the entire project at that moment in time.</p> <p>This might sound inefficient, but Git is clever about storage. Unchanged files aren't duplicated; Git just points to the previous version. And for storage efficiency, Git periodically packs objects and stores deltas. But conceptually, each commit represents a complete picture of the project.</p> <p>This snapshot model makes certain operations much simpler. Switching between branches or commits means loading a different snapshot, not replaying a series of patches. Comparing any two points in history is straightforward\u2014you're comparing two snapshots, not computing the cumulative effect of all changes between them.</p>"},{"location":"concepts/version-control/#the-three-states","title":"The Three States","text":"<p>In Git, files exist in one of three states:</p> <ul> <li>Modified. You've changed the file but haven't recorded the change yet.</li> <li>Staged. You've marked a modified file to go into your next commit.</li> <li>Committed. The change is safely stored in your local repository.</li> </ul> <p>This three-stage process might seem unnecessarily complex compared to a simple \"save\" operation. But the staging area gives you control over exactly what goes into each commit. You might have five modified files, but only three of them are related to the bug you're fixing. Stage those three, commit them as a logical unit, then deal with the other two separately.</p> <p>Clean, focused commits make history easier to understand and problems easier to diagnose. The staging area is the mechanism that makes this practical.</p>"},{"location":"concepts/version-control/#commits-and-the-dag","title":"Commits and the DAG","text":"<p>Each commit in Git records:</p> <ul> <li>A snapshot of all tracked files</li> <li>A pointer to the parent commit(s)</li> <li>Author and committer information</li> <li>A timestamp</li> <li>A commit message</li> </ul> <p>Commits are identified by a SHA-1 hash\u2014a 40-character string computed from the commit's contents. This hash is essentially a fingerprint: any change to the commit (content, message, parent, anything) would produce a different hash.</p> <p>The parent pointers create a directed acyclic graph (DAG) of commits. Most commits have one parent\u2014the commit that came before. Merge commits have two parents\u2014the commits from each branch being combined. The first commit in a repository has no parent.</p> <p>This graph structure is fundamental to how Git works. Branches are just pointers to commits. The history of a branch is the chain of commits you can reach by following parent pointers. Merging combines histories by creating a commit with multiple parents.</p>"},{"location":"concepts/version-control/#branches-are-cheap","title":"Branches Are Cheap","text":"<p>In older systems, creating a branch might copy files, take significant time, and create pressure to avoid branching unnecessarily. In Git, a branch is literally a 41-byte file containing a commit hash.</p> <p>Creating a branch is instantaneous. Switching between branches is fast. Having dozens of branches costs almost nothing. This changes how you work.</p> <p>In a world where branches are expensive, you work on the main line and branch only when necessary\u2014for releases, for risky experiments. Merging is infrequent and often painful.</p> <p>In a world where branches are cheap, you branch constantly. Every feature, every bug fix, every experiment gets its own branch. You work in isolation until ready, then merge back. The main branch stays stable because incomplete work lives elsewhere.</p> <p>This model\u2014often called \"feature branching\" or \"topic branching\"\u2014has become the standard workflow for software development. It's practical only because Git makes branching nearly free.</p>"},{"location":"concepts/version-control/#why-version-control-matters","title":"Why Version Control Matters","text":""},{"location":"concepts/version-control/#history-as-documentation","title":"History as Documentation","text":"<p>Every commit is a record: what changed, who changed it, when, and why (if the commit message is good). This history is invaluable for understanding a system.</p> <p>When you encounter confusing code, you can trace its history. When did it appear? Who wrote it? What was the commit message? Is there an associated issue or pull request with discussion? Often the \"why\" that's missing from the code itself is preserved in the version control history.</p> <p>This historical record also serves compliance and audit needs. In regulated industries, being able to demonstrate who changed what and when is a requirement. Version control provides this automatically.</p>"},{"location":"concepts/version-control/#fearless-experimentation","title":"Fearless Experimentation","text":"<p>Without version control, changing working code is risky. What if you break something? What if you can't get back to the working state? This fear leads to stagnation\u2014code that nobody wants to touch because it might break.</p> <p>Version control eliminates this fear. You can always get back to any previous state. Try something radical; if it doesn't work, revert. Create a branch for an experiment; if it fails, delete it. The safety net of version control enables the experimentation that leads to better code.</p>"},{"location":"concepts/version-control/#collaboration-without-chaos","title":"Collaboration Without Chaos","text":"<p>Software projects involve multiple people changing the same codebase. Without coordination, this leads to chaos\u2014people overwriting each other's work, changes getting lost, incompatible modifications creating bugs.</p> <p>Version control provides the coordination. It tracks who changed what. It merges concurrent changes automatically when possible and flags conflicts when not. It provides a shared history that everyone can reference.</p> <p>This coordination scales. Small teams can work together easily. Large open source projects with thousands of contributors across the globe function because version control manages the complexity.</p>"},{"location":"concepts/version-control/#the-foundation-for-automation","title":"The Foundation for Automation","text":"<p>Version control is the foundation for modern software development practices:</p> <ul> <li>Continuous integration triggers automated builds and tests when code is pushed.</li> <li>Code review happens on pull requests before changes merge.</li> <li>Deployment pipelines deploy code from specific branches or tags.</li> <li>Infrastructure as code stores configuration in version control and applies it automatically.</li> </ul> <p>None of these practices would be practical without version control as the underlying mechanism for tracking and triggering on changes.</p>"},{"location":"concepts/version-control/#the-social-dimension","title":"The Social Dimension","text":"<p>The rise of platforms like GitHub added a social layer to version control that changed how people work and collaborate.</p>"},{"location":"concepts/version-control/#pull-requests-and-code-review","title":"Pull Requests and Code Review","text":"<p>A pull request (or merge request) is a proposal to merge changes from one branch into another. But it's more than just a merge mechanism\u2014it's a collaboration tool.</p> <p>Pull requests provide a place for discussion. Reviewers can comment on specific lines of code. Authors can respond and make changes. The conversation is preserved alongside the code, providing context for future readers.</p> <p>Code review has become standard practice not because tools forced it, but because pull requests made it natural. The barrier to reviewing code dropped, and the benefits became obvious.</p>"},{"location":"concepts/version-control/#open-source-collaboration","title":"Open Source Collaboration","text":"<p>Distributed version control made large-scale open source collaboration practical. Anyone can clone a repository, make changes, and propose them back. Maintainers can review and merge contributions from strangers without giving them direct access.</p> <p>This model\u2014fork, modify, propose\u2014has enabled an explosion of open source software. Projects can accept contributions from thousands of developers without security concerns or coordination overhead.</p>"},{"location":"concepts/version-control/#profiles-and-portfolios","title":"Profiles and Portfolios","text":"<p>Version control history is public on platforms like GitHub. Your contributions are visible: what projects you've worked on, how much you've contributed, the quality of your commits and pull requests.</p> <p>For developers, this visibility serves as a portfolio. For employers, it's a signal about candidates. For projects, it's a measure of contributor reputation.</p> <p>This visibility cuts both ways. Good work is rewarded with reputation. But the pressure to show activity can lead to gaming metrics or feeling inadequate compared to those with more public contributions.</p>"},{"location":"concepts/version-control/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"concepts/version-control/#meaningless-commit-messages","title":"Meaningless Commit Messages","text":"<p>The commit message is your chance to explain why a change was made. Messages like \"fix bug,\" \"update code,\" or \"WIP\" waste this opportunity. When someone (including future you) needs to understand the change, a meaningless message provides no help.</p> <p>Good commit messages describe intent, not just action. Not \"change timeout to 30\" but \"increase timeout to handle slow network responses.\" The diff shows what changed; the message should explain why.</p>"},{"location":"concepts/version-control/#commits-that-are-too-large","title":"Commits That Are Too Large","text":"<p>A commit that changes dozens of files across multiple unrelated features is hard to understand, hard to review, and hard to revert if something goes wrong.</p> <p>Smaller, focused commits are easier to work with. Each commit should represent a single logical change. If you need to explain your commit with \"and,\" it might be too big.</p>"},{"location":"concepts/version-control/#ignoring-history","title":"Ignoring History","text":"<p>Version control captures history, but that history is only useful if you reference it. Many developers never run <code>git log</code>, never use <code>git blame</code>, never explore how the code evolved.</p> <p>The history is a resource. When debugging, check when the problematic code was introduced. When reviewing code, look at the history of the file. When onboarding, explore the commit history to understand how the project developed.</p>"},{"location":"concepts/version-control/#fear-of-branching","title":"Fear of Branching","text":"<p>Developers coming from centralized systems sometimes avoid branches out of habit. They commit directly to main, or they accumulate large changes locally before committing.</p> <p>In Git, branches are cheap and merging usually works well. Use them freely. Branch for features, for experiments, for anything you're not sure about. The cost is near zero; the benefits are significant.</p>"},{"location":"concepts/version-control/#version-control-beyond-code","title":"Version Control Beyond Code","text":"<p>Version control originated in software development, but the principles apply anywhere you're managing changing files:</p> <ul> <li>Documentation benefits from version control. Technical writers can track changes, collaborate on drafts, and maintain multiple versions for different releases.</li> <li>Configuration files for infrastructure should be version controlled. This is the foundation of infrastructure as code.</li> <li>Data science workflows can track experiments, parameters, and notebooks. Specialized tools extend Git for large data files.</li> <li>Legal documents go through revisions. Version control can track changes more precisely than Word's track changes.</li> <li>Writing projects of any kind can benefit from the ability to experiment freely and maintain history.</li> </ul> <p>The core insight\u2014that tracking changes over time is valuable\u2014applies far beyond code. As version control tools become more accessible, these practices spread to new domains.</p>"},{"location":"git-101/","title":"Git 101","text":"<p>You've probably been there: a config file that worked yesterday is broken today, and nobody knows what changed. Or you're afraid to touch a script because there's no way to undo it. Or you're emailing files back and forth with <code>-v2-final-FINAL.txt</code> in the name.</p> <p>Git solves these problems. It tracks every change you make, who made it, and why\u2014so you can experiment freely, collaborate without stepping on each other, and always get back to a known-good state.</p>"},{"location":"git-101/#why-you-should-care","title":"Why You Should Care","text":"<p>If you manage configs, scripts, documentation, or infrastructure-as-code, Git gives you:</p> <ul> <li>A real undo button. Made a bad change? Roll back to any previous version in seconds.</li> <li>Fearless experimentation. Try risky changes on a branch. If it works, merge it. If not, delete it. Your main copy stays safe.</li> <li>Accountability without blame. When something breaks at 2 AM, <code>git log</code> and <code>git blame</code> tell you exactly what changed and when\u2014so you can fix it instead of guessing.</li> <li>Collaboration that scales. Multiple people can work on the same files without overwriting each other's work.</li> </ul>"},{"location":"git-101/#lessons","title":"Lessons","text":"<p>Work through these in order:</p> <ol> <li>Local Foundations \u2014 Set up Git, create a repo, make your first commits</li> <li>Understanding History \u2014 Read logs, compare changes, ignore files you don't want tracked</li> <li>Branching and Merging \u2014 Work on isolated changes and combine them safely</li> <li>Working with Remotes \u2014 Sync with shared repositories (push, pull, clone)</li> <li>Recovery and Confidence \u2014 Fix mistakes and get unstuck</li> </ol>"},{"location":"git-101/#tips-for-success","title":"Tips for Success","text":"<ul> <li>Use a scratch repo. Create a throwaway folder to experiment in. Break things on purpose. Delete it and start over when you get stuck\u2014that's the fastest way to learn.</li> <li>Run <code>git status</code> constantly. Before and after every operation. It tells you exactly what Git sees, and it's the quickest way to stay oriented.</li> <li>Don't memorize\u2014understand. Git has a lot of commands, but they all revolve around a few core concepts. Once those click, the commands make sense.</li> </ul>"},{"location":"git-101/lesson-01-local-foundations/","title":"Local Foundations","text":"<p>By the end of this section, you'll have Git configured, a repository created, and your first commits under your belt.</p>"},{"location":"git-101/lesson-01-local-foundations/#configure-your-identity","title":"Configure Your Identity","text":"<p>Before you make any commits, Git needs to know who you are. This information appears in every commit you make\u2014it's how your team knows who changed what.</p> <p>If you haven't already, complete the Initial Configuration steps in the Git tool guide. This sets your name and email globally so you don't have to configure it for each repository.</p> <p>Global vs Local Config</p> <p>The <code>--global</code> flag sets these values for all repositories on your machine. You can override them per-repository by running the same commands without <code>--global</code> from inside a repo.</p>"},{"location":"git-101/lesson-01-local-foundations/#create-your-first-repository","title":"Create Your First Repository","text":"<p>Let's create a scratch repository you can experiment with freely.</p> <pre><code>mkdir -p ~/scratch/git-sandbox\ncd ~/scratch/git-sandbox\n</code></pre> <p>Now initialize Git:</p> <pre><code>git init\n</code></pre> <p>You'll see: <code>Initialized empty Git repository in .../git-sandbox/.git/</code></p> <p>That <code>.git</code> folder is where Git stores everything\u2014all your history, branches, and metadata. You'll never need to touch it directly, but knowing it exists helps demystify what Git is doing.</p> <p>Run your first status check:</p> <pre><code>git status\n</code></pre> <p>You'll see something like:</p> <pre><code>On branch main\nNo commits yet\nnothing to commit (create/copy some files and use \"git add\" to track)\n</code></pre> <p>This tells you three things:</p> <ol> <li>You're on a branch called <code>main</code> (we'll cover branches later)</li> <li>There are no commits yet</li> <li>There's nothing to commit because the folder is empty</li> </ol>"},{"location":"git-101/lesson-01-local-foundations/#the-stage-and-commit-workflow","title":"The Stage and Commit Workflow","text":"<p>Here's where Git differs from simple backup systems. Git doesn't automatically track every file or save every change. You explicitly tell it what to record. This happens in two steps:</p> <ol> <li>Stage \u2014 Select which changes you want to include in your next commit</li> <li>Commit \u2014 Save those staged changes as a permanent snapshot</li> </ol> <p>Think of it like packing a box before shipping it. You don't throw everything in at once\u2014you choose what goes in the box (staging), then seal and label it (commit).</p>"},{"location":"git-101/lesson-01-local-foundations/#create-a-file-and-check-status","title":"Create a file and check status","text":"<pre><code>echo \"# Server Notes\" &gt; notes.md\ngit status\n</code></pre> <p>Git now shows <code>notes.md</code> as an \"untracked file.\" Git sees it exists but isn't tracking changes to it yet.</p>"},{"location":"git-101/lesson-01-local-foundations/#stage-the-file","title":"Stage the file","text":"<pre><code>git add notes.md\ngit status\n</code></pre> <p>Now <code>notes.md</code> appears under \"Changes to be committed.\" It's staged\u2014ready to be included in your next commit.</p> <p>Stage multiple files at once</p> <p><code>git add .</code> stages all changes in the current directory. Useful, but be careful\u2014make sure you're not staging files you don't want (like secrets or build artifacts). Always run <code>git status</code> first.</p>"},{"location":"git-101/lesson-01-local-foundations/#commit-the-staged-changes","title":"Commit the staged changes","text":"<pre><code>git commit -m \"Add server notes file\"\n</code></pre> <p>Done. You've created your first commit. Git responds with a summary:</p> <pre><code>[main (root-commit) a1b2c3d] Add server notes file\n 1 file changed, 1 insertion(+)\n create mode 100644 notes.md\n</code></pre> <p>The string <code>a1b2c3d</code> is an abbreviated commit hash\u2014a unique identifier for this exact snapshot. Every commit gets one.</p>"},{"location":"git-101/lesson-01-local-foundations/#the-full-cycle","title":"The full cycle","text":"<p>Let's do it again to reinforce the pattern. Edit the file:</p> <pre><code>echo \"Reboot procedure: sudo reboot\" &gt;&gt; notes.md\n</code></pre> <p>Check what changed:</p> <pre><code>git status\n</code></pre> <p>Git shows <code>notes.md</code> as \"modified.\" It's already tracked, so Git notices the change, but that change isn't staged yet.</p> <p>See exactly what changed:</p> <pre><code>git diff\n</code></pre> <p>This shows the lines added (prefixed with <code>+</code>) and removed (prefixed with <code>-</code>). Get comfortable with <code>git diff</code>\u2014it's your preview before committing.</p> <p>Stage and commit:</p> <pre><code>git add notes.md\ngit commit -m \"Add reboot procedure to server notes\"\n</code></pre>"},{"location":"git-101/lesson-01-local-foundations/#why-two-steps","title":"Why two steps?","text":"<p>The staging area lets you craft clean, logical commits even when your working directory is messy. You might have three files changed, but only two are related to the same task. Stage those two, commit them, then stage and commit the third separately.</p> <p>Clean commits make history easier to read and problems easier to diagnose. When something breaks, you want to find \"Add SSL config\" in your history\u2014not \"Various changes and fixes.\"</p>"},{"location":"git-101/lesson-01-local-foundations/#writing-good-commit-messages","title":"Writing Good Commit Messages","text":"<p>Commit messages are for your future self and your teammates. Write them like you'll be the one debugging this at 2 AM six months from now.</p>"},{"location":"git-101/lesson-01-local-foundations/#use-imperative-mood","title":"Use imperative mood","text":"<p>Write messages as commands: \"Add config file\" not \"Added config file\" or \"Adding config file.\"</p> <p>This matches Git's own conventions (e.g., \"Merge branch 'feature'\") and reads naturally when you look at history: \"If I apply this commit, it will Add config file.\"</p> <p>Good:</p> <ul> <li><code>Add SSL certificate rotation script</code></li> <li><code>Fix incorrect timeout in backup job</code></li> <li><code>Remove deprecated API endpoint</code></li> </ul> <p>Bad:</p> <ul> <li><code>Added some stuff</code></li> <li><code>WIP</code></li> <li><code>asdfasdf</code></li> <li><code>Fixed it</code></li> </ul>"},{"location":"git-101/lesson-01-local-foundations/#be-specific","title":"Be specific","text":"<p>Your message should explain what changed and, briefly, why. Someone reading the log should understand the intent without reading the code.</p> Instead of... Write... <code>Update config</code> <code>Increase nginx worker connections to 4096</code> <code>Fix bug</code> <code>Fix race condition in session cleanup</code> <code>Changes</code> <code>Add retry logic to LDAP connection</code>"},{"location":"git-101/lesson-01-local-foundations/#keep-the-first-line-short","title":"Keep the first line short","text":"<p>The first line should be under 50 characters\u2014this is what appears in logs, GitHub, and most Git tools. If you need more detail, add a blank line and then a longer description:</p> <pre><code>git commit -m \"Add retry logic to LDAP connection\n\nPrevious implementation failed silently on network timeouts.\nNow retries 3 times with exponential backoff before failing.\"\n</code></pre>"},{"location":"git-101/lesson-01-local-foundations/#exercises","title":"Exercises","text":"<ol> <li> <p>Practice the cycle: Create two more files in your sandbox, stage them separately, and commit each with a descriptive message. Run <code>git status</code> before and after each command to see how the state changes.</p> </li> <li> <p>Selective staging: Modify two files, but only stage and commit one. Verify the other file still shows as modified after the commit.</p> </li> <li> <p>View your history: Run <code>git log</code> to see your commits. Try <code>git log --oneline</code> for a compact view. We'll dig deeper into history in the next section.</p> </li> </ol>"},{"location":"git-101/lesson-02-understanding-history/","title":"Understanding History","text":"<p>Goal: Read your project's history, compare changes, and keep your repository clean.</p>"},{"location":"git-101/lesson-02-understanding-history/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>Navigate commit history with <code>git log</code></li> <li>Compare changes with <code>git diff</code></li> <li>Use <code>.gitignore</code> for secrets, build artifacts, and OS clutter</li> <li>Find who changed what with <code>git blame</code></li> </ul>"},{"location":"git-101/lesson-02-understanding-history/#reading-the-commit-log","title":"Reading the Commit Log","text":"<p>TODO</p>"},{"location":"git-101/lesson-02-understanding-history/#comparing-changes-with-diff","title":"Comparing Changes with Diff","text":"<p>TODO</p>"},{"location":"git-101/lesson-02-understanding-history/#ignoring-files-with-gitignore","title":"Ignoring Files with .gitignore","text":"<p>TODO</p>"},{"location":"git-101/lesson-02-understanding-history/#finding-who-changed-what","title":"Finding Who Changed What","text":"<p>TODO</p>"},{"location":"git-101/lesson-02-understanding-history/#exercises","title":"Exercises","text":"<p>TODO</p>"},{"location":"git-101/lesson-03-branching-and-merging/","title":"Branching and Merging","text":"<p>Goal: Work on isolated changes and combine them back together.</p>"},{"location":"git-101/lesson-03-branching-and-merging/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>Create and switch between branches</li> <li>Merge branches and handle simple conflicts</li> <li>Decide when to branch vs. commit directly</li> </ul>"},{"location":"git-101/lesson-03-branching-and-merging/#why-branches-exist","title":"Why Branches Exist","text":"<p>TODO</p>"},{"location":"git-101/lesson-03-branching-and-merging/#creating-and-switching-branches","title":"Creating and Switching Branches","text":"<p>TODO</p>"},{"location":"git-101/lesson-03-branching-and-merging/#merging-branches","title":"Merging Branches","text":"<p>TODO</p>"},{"location":"git-101/lesson-03-branching-and-merging/#handling-merge-conflicts","title":"Handling Merge Conflicts","text":"<p>TODO</p>"},{"location":"git-101/lesson-03-branching-and-merging/#when-to-branch","title":"When to Branch","text":"<p>TODO</p>"},{"location":"git-101/lesson-03-branching-and-merging/#exercises","title":"Exercises","text":"<p>TODO</p>"},{"location":"git-101/lesson-04-working-with-remotes/","title":"Working with Remotes","text":"<p>Goal: Sync your work with a shared repository.</p>"},{"location":"git-101/lesson-04-working-with-remotes/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>Clone existing repositories</li> <li>Push your changes and pull others' changes</li> <li>Safe daily habits for working with remotes</li> </ul>"},{"location":"git-101/lesson-04-working-with-remotes/#what-is-a-remote","title":"What Is a Remote?","text":"<p>TODO</p>"},{"location":"git-101/lesson-04-working-with-remotes/#cloning-a-repository","title":"Cloning a Repository","text":"<p>TODO</p>"},{"location":"git-101/lesson-04-working-with-remotes/#push-pull-and-fetch","title":"Push, Pull, and Fetch","text":"<p>TODO</p>"},{"location":"git-101/lesson-04-working-with-remotes/#safe-habits-for-shared-repositories","title":"Safe Habits for Shared Repositories","text":"<p>TODO</p>"},{"location":"git-101/lesson-04-working-with-remotes/#exercises","title":"Exercises","text":"<p>TODO</p>"},{"location":"git-101/lesson-05-recovery-and-confidence/","title":"Recovery and Confidence","text":"<p>Goal: Fix mistakes and get unstuck without panic.</p>"},{"location":"git-101/lesson-05-recovery-and-confidence/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>Undo uncommitted changes</li> <li>Amend recent commits</li> <li>Safe ways to recover when things go wrong</li> </ul>"},{"location":"git-101/lesson-05-recovery-and-confidence/#undoing-uncommitted-changes","title":"Undoing Uncommitted Changes","text":"<p>TODO</p>"},{"location":"git-101/lesson-05-recovery-and-confidence/#amending-recent-commits","title":"Amending Recent Commits","text":"<p>TODO</p>"},{"location":"git-101/lesson-05-recovery-and-confidence/#common-i-messed-up-scenarios","title":"Common \"I Messed Up\" Scenarios","text":"<p>TODO</p>"},{"location":"git-101/lesson-05-recovery-and-confidence/#exercises","title":"Exercises","text":"<p>TODO</p>"},{"location":"git-101/prerequisites/","title":"Prerequisites","text":""},{"location":"git-101/prerequisites/#required-tools","title":"Required Tools","text":"Tool Why You Need It Terminal You'll run all Git commands here. macOS/Linux: use the built-in terminal. Windows: use WSL. Git The version control system this module teaches"},{"location":"git-101/prerequisites/#recommended-tools","title":"Recommended Tools","text":"Tool Why It Helps Visual Studio Code Makes merge conflicts much easier to resolve with its built-in diff viewer"},{"location":"git-101/prerequisites/#required-modules","title":"Required Modules","text":"<p>None. This is a foundational module.</p>"},{"location":"puppet-101/","title":"Puppet 101","text":"<p>TODO: Overview of Puppet fundamentals.</p>"},{"location":"puppet-101/prerequisites/","title":"Prerequisites","text":""},{"location":"puppet-101/prerequisites/#required-tools","title":"Required Tools","text":"Tool Why You Need It TODO TODO"},{"location":"puppet-101/prerequisites/#required-modules","title":"Required Modules","text":"<p>None. This is a foundational module.</p>"},{"location":"puppet-201/","title":"Puppet 201","text":"<p>TODO: Overview of advanced Puppet topics.</p>"},{"location":"puppet-201/prerequisites/","title":"Prerequisites","text":""},{"location":"puppet-201/prerequisites/#required-tools","title":"Required Tools","text":"Tool Why You Need It TODO TODO"},{"location":"puppet-201/prerequisites/#required-modules","title":"Required Modules","text":"Module Why It's Required Puppet 101 Covers Puppet fundamentals Git 101 Puppet code is managed with Git"},{"location":"tools/","title":"Tools","text":"<p>Reference guides for installing and configuring the tools used across our training modules.</p> <p>Each guide covers installation, basic setup, and tips for getting the most out of the tool. You don't need to install everything upfront\u2014check the prerequisites for each module to see what's required.</p>"},{"location":"tools/#available-tools","title":"Available Tools","text":"Tool Platform Description Homebrew macOS Package manager for installing CLI tools Windows Terminal Windows Modern terminal with tabs, profiles, and WSL integration WSL Windows Run Linux on Windows\u2014recommended for development Visual Studio Code All Code editor with Git integration and remote development Git All Version control system Puppet Development Kit (PDK) All Build and test Puppet modules"},{"location":"tools/git/","title":"Git","text":"<p>Git is a version control system that tracks changes to files over time. You save checkpoints (commits), work on isolated changes (branches), and combine work together (merges). It's the foundation for modern infrastructure-as-code, config management, and collaborative workflows.</p>"},{"location":"tools/git/#installation","title":"Installation","text":""},{"location":"tools/git/#macos","title":"macOS","text":"<p>Homebrew (recommended)</p> <pre><code>brew install git\n</code></pre>"},{"location":"tools/git/#linux-ubuntu-wsl","title":"Linux (Ubuntu) / WSL","text":"<pre><code>sudo apt update &amp;&amp; sudo apt install -y git\n</code></pre> <p>Windows users</p> <p>Install Git inside WSL, not on Windows itself. This keeps your tooling consistent with the Linux environments you'll deploy to.</p>"},{"location":"tools/git/#verify-installation","title":"Verify Installation","text":"<pre><code>git --version\n</code></pre> <p>You should see something like <code>git version 2.x.x</code>.</p>"},{"location":"tools/git/#initial-configuration","title":"Initial Configuration","text":"<p>Set these once per machine. They identify you in every commit you make.</p> <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"you@uoregon.edu\"\n</code></pre> <p>Use your real name and work email. When someone runs <code>git log</code> or <code>git blame</code>, your name/email is what they'll see.</p> <p>Recommended settings:</p> <pre><code>git config --global pull.rebase true    # cleaner history when pulling changes\n</code></pre> <p>Verify your config:</p> <pre><code>git config --global --list\n</code></pre>"},{"location":"tools/homebrew/","title":"Homebrew (macOS)","text":"<p>Homebrew is a package manager that makes it easy to install command line tools on macOS.</p> <ul> <li>Consistent installs for CLI tools across machines.</li> <li>Easy updates: <code>brew update &amp;&amp; brew upgrade</code>.</li> <li>Dependency management: <code>brew info &lt;package&gt;</code> shows versions and requirements.</li> </ul>"},{"location":"tools/homebrew/#prerequisites","title":"Prerequisites","text":"<p>Ensure Command Line Tools are installed:</p> <pre><code>xcode-select --install\n</code></pre>"},{"location":"tools/homebrew/#installation","title":"Installation","text":"<ul> <li>Run the official installer:</li> </ul> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <ul> <li>Add Homebrew to your shell path (follow the installer's final hints)</li> </ul> <pre><code>echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"' &gt;&gt; ~/.zprofile\neval \"$(/opt/homebrew/bin/brew shellenv)\"\n</code></pre> <ul> <li>Verify:</li> </ul> <pre><code>brew --version\nbrew doctor\n</code></pre>"},{"location":"tools/pdk/","title":"Puppet Development Kit (PDK)","text":"<p>PDK is Puppet's official toolkit for developing and testing Puppet modules. It provides a consistent environment for creating modules, writing unit tests, validating syntax, and packaging modules for publication. PDK also includes a language server that integrates with editors like VS Code, giving you autocompletion, real-time validation, and inline documentation as you write Puppet code. If you're writing Puppet code, PDK standardizes your workflow and catches errors before they hit production.</p>"},{"location":"tools/pdk/#puppet-forge-account","title":"Puppet Forge Account","text":"<p>You need a Puppet Forge account to download PDK. The download requires accepting Puppet's EULA, which is tied to your Forge account. You'll also use this account later to publish modules and generate API keys.</p> <ol> <li>Go to forge.puppet.com</li> <li>Click Sign Up in the top right</li> <li>Create an account using your work email</li> <li>Verify your email address</li> </ol>"},{"location":"tools/pdk/#forge-api-key","title":"Forge API Key","text":"<p>An API key lets you authenticate with the Forge from the command line, which is required for publishing modules.</p> <ol> <li>Log in to forge.puppet.com</li> <li>Click your username in the top right \u2192 Account Settings</li> <li>Select API Keys from the sidebar</li> <li>Click Generate API Key</li> <li>Give the key a descriptive name (e.g., \"workstation-pdk\")</li> <li>Copy the key immediately\u2014you won't be able to see it again</li> </ol> <p>Store the key securely. You'll use it when publishing modules:</p> <pre><code>pdk release --forge-token=&lt;your-api-key&gt;\n</code></pre> <p>Or set it as an environment variable to avoid passing it on every command:</p> <pre><code>export PDK_FORGE_TOKEN=&lt;your-api-key&gt;\n</code></pre> <p>Add this to your shell profile (<code>~/.bashrc</code>, <code>~/.zshrc</code>) to make it permanent.</p>"},{"location":"tools/pdk/#installation","title":"Installation","text":""},{"location":"tools/pdk/#macos","title":"macOS","text":"<p>Homebrew (recommended)</p> <pre><code>brew install --cask puppetlabs/puppet/pdk\n</code></pre>"},{"location":"tools/pdk/#windows","title":"Windows","text":"<p>Download and run the MSI installer from the official Puppet downloads page:</p> <ol> <li>Go to puppet.com/try-puppet/puppet-development-kit</li> <li>Download the Windows (.msi) package</li> <li>Run the installer and follow the prompts</li> <li>Open a new PowerShell or Command Prompt window after installation</li> </ol> <p>PDK will be added to your PATH automatically.</p> <p>WSL users</p> <p>Install PDK in Windows, not inside WSL. PDK is primarily a Windows/macOS tool and works best natively. You can still edit your module code in WSL and use PDK from PowerShell for validation and testing.</p>"},{"location":"tools/pdk/#linux-ubuntudebian","title":"Linux (Ubuntu/Debian)","text":"<p>Install from Puppet's official apt repository:</p> <pre><code># Download and install the Puppet release package\nwget https://apt.puppet.com/puppet-tools-release-$(lsb_release -cs).deb\nsudo dpkg -i puppet-tools-release-$(lsb_release -cs).deb\nsudo apt update\n\n# Install PDK\nsudo apt install pdk\n</code></pre>"},{"location":"tools/pdk/#linux-rhelcentosfedora","title":"Linux (RHEL/CentOS/Fedora)","text":"<p>Install from Puppet's official yum repository:</p> <pre><code># Install the Puppet release package\nsudo rpm -Uvh https://yum.puppet.com/puppet-tools-release-el-$(rpm -E %rhel).noarch.rpm\n\n# Install PDK\nsudo yum install pdk\n</code></pre>"},{"location":"tools/pdk/#verify-installation","title":"Verify Installation","text":"<pre><code>pdk --version\n</code></pre> <p>You should see output like <code>2.x.x</code>.</p> <p>Test that PDK can create a new module:</p> <pre><code>pdk new module test_module --skip-interview\n</code></pre> <p>This creates a <code>test_module</code> directory with the standard Puppet module structure.</p>"},{"location":"tools/pdk/#essential-commands","title":"Essential Commands","text":"Command Description <code>pdk new module &lt;name&gt;</code> Create a new module with standard structure <code>pdk new class &lt;name&gt;</code> Add a new class to the current module <code>pdk new defined_type &lt;name&gt;</code> Add a new defined type <code>pdk new task &lt;name&gt;</code> Add a new Bolt task <code>pdk validate</code> Check syntax and style (metadata, Puppet, Ruby) <code>pdk test unit</code> Run unit tests <code>pdk build</code> Package the module for publishing <code>pdk release</code> Publish the module to the Forge"},{"location":"tools/pdk/#editor-integration","title":"Editor Integration","text":"<p>PDK includes a language server that provides intelligent editing features when working with Puppet code. Editors like VS Code can connect to this language server to offer:</p> <ul> <li>Autocompletion \u2014 suggests resource types, parameters, and variables as you type</li> <li>Hover documentation \u2014 displays inline help for resources, functions, and parameters</li> <li>Real-time validation \u2014 highlights syntax errors and style issues without leaving your editor</li> <li>Go to definition \u2014 jump to class and defined type declarations across your module</li> <li>Code snippets \u2014 insert boilerplate for common patterns like resource declarations</li> </ul> <p>This feedback loop catches mistakes early\u2014before you run <code>pdk validate</code> or push code. Instead of writing Puppet manifests blind and discovering problems later, you get immediate guidance on correct syntax, available parameters, and type mismatches.</p>"},{"location":"tools/pdk/#vs-code-setup","title":"VS Code Setup","text":"<p>Install the Puppet extension from the VS Code marketplace. The extension automatically detects PDK and uses it for validation and language features.</p> <p>After installation:</p> <ol> <li>Open a PDK-generated module folder in VS Code</li> <li>Look for \"Puppet\" in the status bar (bottom right) to confirm the language server is running</li> <li>Open any <code>.pp</code> file to see autocompletion and validation in action</li> </ol> <p>Tip</p> <p>The extension works best when you open the module root folder (the one containing <code>metadata.json</code>) rather than individual files or parent directories.</p>"},{"location":"tools/pdk/#useful-configuration","title":"Useful Configuration","text":""},{"location":"tools/pdk/#skip-the-interview","title":"Skip the interview","text":"<p>When creating new modules or classes, PDK prompts you for metadata interactively. Skip this with:</p> <pre><code>pdk new module my_module --skip-interview\n</code></pre> <p>You can edit <code>metadata.json</code> manually afterward.</p>"},{"location":"tools/pdk/#update-module-templates","title":"Update module templates","text":"<p>PDK uses templates to generate module scaffolding. Update to the latest templates:</p> <pre><code>pdk update\n</code></pre> <p>This updates your module's CI configs, testing setup, and other boilerplate to match current best practices.</p>"},{"location":"tools/pdk/#configure-default-answers","title":"Configure default answers","text":"<p>Create <code>~/.pdk/analytics.yml</code> and <code>~/.pdk/cache/answers.json</code> to store default values for the interview questions, so you don't have to re-enter them for every new module.</p>"},{"location":"tools/vscode/","title":"Visual Studio Code","text":"<p>Visual Studio Code is a lightweight editor with strong Git support and useful extensions.</p>"},{"location":"tools/vscode/#windows","title":"Windows","text":"<ul> <li>Install via Winget (recommended):</li> </ul> <pre><code>winget install --id Microsoft.VisualStudioCode -e\n</code></pre> <ul> <li>Verify:</li> </ul> <pre><code>code --version\n</code></pre> <ul> <li>If <code>code</code> is not in PATH, launch VS Code, press <code>Ctrl+Shift+P</code>, run \"Shell Command: Install 'code' command in PATH\", then rerun <code>code --version</code>.</li> </ul>"},{"location":"tools/vscode/#macos","title":"macOS","text":"<ul> <li>Install via Homebrew:</li> </ul> <pre><code>brew install --cask visual-studio-code\n</code></pre> <ul> <li>Verify:</li> </ul> <pre><code>code --version\n</code></pre> <ul> <li>If <code>code</code> is not found, open VS Code, press <code>Cmd+Shift+P</code>, and run \"Shell Command: Install 'code' command in PATH\".</li> </ul>"},{"location":"tools/vscode/#linux-debianubuntu","title":"Linux (Debian/Ubuntu)","text":"<ul> <li>Install Microsoft\u2019s repo and package:</li> </ul> <pre><code>sudo apt update\nsudo apt install -y wget gpg\nwget -qO- https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor &gt; packages.microsoft.gpg\nsudo install -D -o root -g root -m 644 packages.microsoft.gpg /etc/apt/keyrings/packages.microsoft.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/packages.microsoft.gpg] https://packages.microsoft.com/repos/code stable main\" | sudo tee /etc/apt/sources.list.d/vscode.list\nsudo apt update\nsudo apt install -y code\ncode --version\n</code></pre> <ul> <li>Clean up the downloaded key file if desired:</li> </ul> <pre><code>rm -f packages.microsoft.gpg\n</code></pre>"},{"location":"tools/vscode/#settings-sync-with-github","title":"Settings Sync with GitHub","text":"<p>I recommend signing into VS Code using your GitHub account as it will allow you to sync your editor settings and profiles so you can have a consistent experience across devices.</p>"},{"location":"tools/vscode/#recommended-settings-and-extensions","title":"Recommended Settings and Extensions","text":""},{"location":"tools/vscode/#settings","title":"Settings","text":"<p>These are a couple of settings I always encourage new users to configure.</p> <p>To access your settings, press <code>Ctrl+Shift+P</code> on Windows/Linux or <code>Cmd+Shift+P</code> on macOS, type \"Settings\", and select \"Preferences: Open Settings (UI)\".</p> <ul> <li>Enable format on save</li> <li>Disable \"compact folders\"</li> <li>Disable \"workbench editor preview\"</li> </ul>"},{"location":"tools/vscode/#extensions","title":"Extensions","text":"<p>VS Code has a huge amount of useful extensions. I've listed some of my recommended extensions below:</p> <ul> <li>Bash IDE (macOS/Linux, bash language server)</li> <li>Powershell (Windows, powershell language server)</li> <li>Error Lens (nice error messages)</li> <li>markdownlint (formatting for Markdown)</li> <li>Prettier - Code formatter (formatter for many filetypes)</li> <li>Puppet (for Puppet modules)</li> <li>Python (Python support)</li> <li>Ruby LSP (Ruby support for working with Puppet)</li> <li>YAML (the one from Red Hat, use for YAML support)</li> </ul>"},{"location":"tools/windows-terminal/","title":"Windows Terminal","text":"<p>Windows Terminal is a modern, tabbed terminal for Windows that handles PowerShell, Command Prompt, and WSL in one app. It supports Unicode, GPU-accelerated text rendering, and custom themes\u2014everything the legacy console lacks.</p>"},{"location":"tools/windows-terminal/#installation","title":"Installation","text":"<p>Option 1: Microsoft Store (recommended\u2014auto-updates)</p> <p>Open the Microsoft Store, search for \"Windows Terminal,\" and install.</p> <p>Option 2: WinGet</p> <pre><code>winget install --id Microsoft.WindowsTerminal -e\n</code></pre>"},{"location":"tools/windows-terminal/#verify-installation","title":"Verify Installation","text":"<p>Launch Windows Terminal from the Start menu. You should see a tabbed interface with PowerShell as the default.</p> <p>Check the version:</p> <pre><code>wt -v\n</code></pre>"},{"location":"tools/windows-terminal/#set-wsl-as-default-profile","title":"Set WSL as Default Profile","text":"<p>If you're using WSL for development, make it your default so new tabs open directly into Linux.</p> <ol> <li>Open Windows Terminal</li> <li>Press <code>Ctrl+,</code> to open Settings (or click the dropdown arrow \u2192 Settings)</li> <li>Under Startup, find Default profile</li> <li>Select Ubuntu (or your WSL distro) from the dropdown</li> <li>Click Save</li> </ol> <p>New tabs and windows now open in WSL by default. You can still open PowerShell or CMD tabs from the dropdown menu.</p>"},{"location":"tools/windows-terminal/#tips","title":"Tips","text":"<ul> <li>Split panes: <code>Alt+Shift+D</code> splits the current tab. Useful for running a command while watching logs.</li> <li>Zoom: <code>Ctrl+=</code> and <code>Ctrl+-</code> adjust font size on the fly.</li> </ul>"},{"location":"tools/wsl/","title":"Windows Subsystem for Linux (WSL)","text":"<p>WSL lets you run a full Linux environment directly on Windows\u2014no virtual machine, no dual-boot. You get a real Linux shell, real Linux tools, and real Linux package managers, all integrated with your Windows filesystem.</p>"},{"location":"tools/wsl/#why-use-wsl-for-development","title":"Why Use WSL for Development?","text":"<p>Most servers, containers, and CI/CD pipelines run Linux. When you develop on Windows but deploy to Linux, you're constantly fighting small differences: path separators, line endings, case sensitivity, shell syntax, missing tools.</p> <p>WSL eliminates this friction:</p> <ul> <li>Same tools as production. Use the same shell, package manager, and utilities you'll find on your servers. No more \"works on my machine\" because your machine is Linux.</li> <li>Native performance. WSL 2 runs a real Linux kernel. File operations, compilation, and container workloads run at near-native speed.</li> <li>Seamless integration. Access Windows files from Linux (<code>/mnt/c/</code>), run Linux commands from PowerShell, and use VS Code's Remote-WSL extension to edit Linux files with full IDE support.</li> <li>Better tooling support. Many development tools (Docker, Kubernetes, Terraform, Ansible, Puppet) are Linux-first. Documentation, tutorials, and Stack Overflow answers assume Linux. Stop translating\u2014just use Linux.</li> </ul> <p>If you're doing infrastructure work, writing scripts, or working with containers, WSL is the path of least resistance on Windows.</p>"},{"location":"tools/wsl/#installation","title":"Installation","text":""},{"location":"tools/wsl/#requirements","title":"Requirements","text":"<ul> <li>Windows 10 version 2004+ (Build 19041+) or Windows 11</li> <li>Virtualization enabled in BIOS (usually on by default)</li> </ul>"},{"location":"tools/wsl/#install-wsl","title":"Install WSL","text":"<p>Open PowerShell as Administrator and run:</p> <pre><code>wsl --install\n</code></pre> <p>This installs WSL 2 with Ubuntu as the default distribution. Restart when prompted.</p> <p>After restart, Ubuntu will launch automatically to complete setup. Create a username and password\u2014this is your Linux user, separate from your Windows account.</p>"},{"location":"tools/wsl/#verify-installation","title":"Verify Installation","text":"<p>Open a new PowerShell window:</p> <pre><code>wsl --version\n</code></pre> <p>You should see WSL version 2.x.x and a kernel version.</p> <p>Launch your Linux environment:</p> <pre><code>wsl\n</code></pre> <p>You're now in a bash shell. Run <code>uname -a</code> to confirm you're in Linux.</p>"},{"location":"tools/wsl/#basic-configuration","title":"Basic Configuration","text":""},{"location":"tools/wsl/#update-packages","title":"Update Packages","text":"<p>First thing after install\u2014update your package list:</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre>"},{"location":"tools/wsl/#install-common-tools","title":"Install Common Tools","text":"<pre><code>sudo apt install -y git curl wget unzip\n</code></pre>"},{"location":"tools/wsl/#access-windows-files","title":"Access Windows Files","text":"<p>Your Windows drives are mounted under <code>/mnt/</code>:</p> <pre><code>cd /mnt/c/Users/YourName/Documents\nls\n</code></pre>"},{"location":"tools/wsl/#access-linux-files-from-windows","title":"Access Linux Files from Windows","text":"<p>Your Linux home directory is accessible from Windows Explorer:</p> <pre><code>\\\\wsl$\\Ubuntu\\home\\yourusername\n</code></pre> <p>Or open Explorer from the Linux terminal:</p> <pre><code>explorer.exe .\n</code></pre>"},{"location":"tools/wsl/#using-wsl-with-vs-code","title":"Using WSL with VS Code","text":"<p>Install the Remote - WSL extension in VS Code.</p> <p>Then from your Linux terminal, open any folder in VS Code:</p> <pre><code>code .\n</code></pre> <p>VS Code connects to WSL and runs extensions inside Linux. You get full IntelliSense, debugging, and terminal access\u2014all running in your Linux environment.</p>"},{"location":"tools/wsl/#tips","title":"Tips","text":"<ul> <li>Keep projects in Linux filesystem. Store code in <code>~/projects</code>, not <code>/mnt/c/</code>. File operations are significantly faster on the Linux filesystem.</li> <li>Use Windows Terminal. It handles WSL, PowerShell, and CMD in tabs with proper Unicode support. See Windows Terminal.</li> <li>One distro is usually enough. Ubuntu is the default and has the best support. You can install multiple distros, but you probably don't need to.</li> </ul>"}]}